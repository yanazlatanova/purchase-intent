{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession connecting to the Spark master\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiDataAnalysis\") \\\n",
    "    .master(\"spark://192.168.1.7:7077\") \\\n",
    "    .config(\"spark.sql.ansi.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable eager evaluation for nice HTML tables in Jupyter\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c3e8162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>Airport_fee</th><th>cbd_congestion_fee</th></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:18:38</td><td>2025-01-01 00:26:59</td><td>1</td><td>1.6</td><td>1</td><td>N</td><td>229</td><td>237</td><td>1</td><td>10.0</td><td>3.5</td><td>0.5</td><td>3.0</td><td>0.0</td><td>1.0</td><td>18.0</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:32:40</td><td>2025-01-01 00:35:13</td><td>1</td><td>0.5</td><td>1</td><td>N</td><td>236</td><td>237</td><td>1</td><td>5.1</td><td>3.5</td><td>0.5</td><td>2.02</td><td>0.0</td><td>1.0</td><td>12.12</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:44:04</td><td>2025-01-01 00:46:01</td><td>1</td><td>0.6</td><td>1</td><td>N</td><td>141</td><td>141</td><td>1</td><td>5.1</td><td>3.5</td><td>0.5</td><td>2.0</td><td>0.0</td><td>1.0</td><td>12.1</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:14:27</td><td>2025-01-01 00:20:01</td><td>3</td><td>0.52</td><td>1</td><td>N</td><td>244</td><td>244</td><td>2</td><td>7.2</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>9.7</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:21:34</td><td>2025-01-01 00:25:06</td><td>3</td><td>0.66</td><td>1</td><td>N</td><td>244</td><td>116</td><td>2</td><td>5.8</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>8.3</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:48:24</td><td>2025-01-01 01:08:26</td><td>2</td><td>2.63</td><td>1</td><td>N</td><td>239</td><td>68</td><td>2</td><td>19.1</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>24.1</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:14:47</td><td>2025-01-01 00:16:15</td><td>0</td><td>0.4</td><td>1</td><td>N</td><td>170</td><td>170</td><td>1</td><td>4.4</td><td>3.5</td><td>0.5</td><td>2.35</td><td>0.0</td><td>1.0</td><td>11.75</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:39:27</td><td>2025-01-01 00:51:51</td><td>0</td><td>1.6</td><td>1</td><td>N</td><td>234</td><td>148</td><td>1</td><td>12.1</td><td>3.5</td><td>0.5</td><td>2.0</td><td>0.0</td><td>1.0</td><td>19.1</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:53:43</td><td>2025-01-01 01:13:23</td><td>0</td><td>2.8</td><td>1</td><td>N</td><td>148</td><td>170</td><td>1</td><td>19.1</td><td>3.5</td><td>0.5</td><td>3.0</td><td>0.0</td><td>1.0</td><td>27.1</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:00:02</td><td>2025-01-01 00:09:36</td><td>1</td><td>1.71</td><td>1</td><td>N</td><td>237</td><td>262</td><td>2</td><td>11.4</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>16.4</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:20:28</td><td>2025-01-01 00:28:04</td><td>1</td><td>2.29</td><td>1</td><td>N</td><td>237</td><td>75</td><td>2</td><td>11.4</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>16.4</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:33:58</td><td>2025-01-01 00:37:23</td><td>1</td><td>0.56</td><td>1</td><td>N</td><td>263</td><td>236</td><td>1</td><td>5.8</td><td>1.0</td><td>0.5</td><td>2.16</td><td>0.0</td><td>1.0</td><td>12.96</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:42:40</td><td>2025-01-01 00:55:38</td><td>3</td><td>1.99</td><td>1</td><td>N</td><td>236</td><td>151</td><td>2</td><td>14.2</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>19.2</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:30:07</td><td>2025-01-01 00:36:48</td><td>1</td><td>1.1</td><td>1</td><td>N</td><td>229</td><td>141</td><td>2</td><td>7.9</td><td>3.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>12.9</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:39:55</td><td>2025-01-01 01:13:59</td><td>1</td><td>3.2</td><td>1</td><td>N</td><td>141</td><td>113</td><td>1</td><td>26.1</td><td>3.5</td><td>0.5</td><td>7.8</td><td>0.0</td><td>1.0</td><td>38.9</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:16:54</td><td>2025-01-01 00:35:12</td><td>3</td><td>2.5</td><td>1</td><td>N</td><td>158</td><td>170</td><td>2</td><td>17.7</td><td>3.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>22.7</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:43:10</td><td>2025-01-01 01:00:03</td><td>1</td><td>1.9</td><td>1</td><td>N</td><td>164</td><td>229</td><td>1</td><td>16.3</td><td>3.5</td><td>0.5</td><td>4.25</td><td>0.0</td><td>1.0</td><td>25.55</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:01:41</td><td>2025-01-01 00:07:14</td><td>1</td><td>0.71</td><td>1</td><td>N</td><td>79</td><td>107</td><td>2</td><td>-7.2</td><td>-1.0</td><td>-0.5</td><td>3.66</td><td>0.0</td><td>-1.0</td><td>-8.54</td><td>-2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-01 00:01:41</td><td>2025-01-01 00:07:14</td><td>1</td><td>0.71</td><td>1</td><td>N</td><td>79</td><td>107</td><td>2</td><td>7.2</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>12.2</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2025-01-01 00:33:12</td><td>2025-01-01 00:50:14</td><td>2</td><td>1.2</td><td>1</td><td>N</td><td>246</td><td>90</td><td>1</td><td>15.6</td><td>3.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>20.6</td><td>2.5</td><td>0.0</td><td>0.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
       "|       1| 2025-01-01 00:18:38|  2025-01-01 00:26:59|              1|          1.6|         1|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:32:40|  2025-01-01 00:35:13|              1|          0.5|         1|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:44:04|  2025-01-01 00:46:01|              1|          0.6|         1|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:14:27|  2025-01-01 00:20:01|              3|         0.52|         1|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:21:34|  2025-01-01 00:25:06|              3|         0.66|         1|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:48:24|  2025-01-01 01:08:26|              2|         2.63|         1|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:14:47|  2025-01-01 00:16:15|              0|          0.4|         1|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:39:27|  2025-01-01 00:51:51|              0|          1.6|         1|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:53:43|  2025-01-01 01:13:23|              0|          2.8|         1|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:00:02|  2025-01-01 00:09:36|              1|         1.71|         1|                 N|         237|         262|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:20:28|  2025-01-01 00:28:04|              1|         2.29|         1|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:33:58|  2025-01-01 00:37:23|              1|         0.56|         1|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:42:40|  2025-01-01 00:55:38|              3|         1.99|         1|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:30:07|  2025-01-01 00:36:48|              1|          1.1|         1|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:39:55|  2025-01-01 01:13:59|              1|          3.2|         1|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:16:54|  2025-01-01 00:35:12|              3|          2.5|         1|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:43:10|  2025-01-01 01:00:03|              1|          1.9|         1|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|       -7.2| -1.0|   -0.5|      3.66|         0.0|                 -1.0|       -8.54|                -2.5|        0.0|               0.0|\n",
       "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|        0.0|               0.0|\n",
       "|       1| 2025-01-01 00:33:12|  2025-01-01 00:50:14|              2|          1.2|         1|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data = spark.read.parquet(\"/opt/spark/data/yellow_tripdata_2025-01.parquet\")\n",
    "taxi_data.createOrReplaceTempView(\"taxi_data\")\n",
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46f4b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/opt/spark/data/yellow_tripdata_2025-02.parquet. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:810)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... (:-1)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:1410)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:152)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:2888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2493)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m taxi_data = spark.read.parquet(\u001b[33m\"\u001b[39m\u001b[33m/opt/spark/data/yellow_tripdata_2025-02.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtaxi_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateOrReplaceTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaxi_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m taxi_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1979\u001b[39m, in \u001b[36mDataFrame.createOrReplaceTempView\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreateOrReplaceTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1976\u001b[39m     command = plan.CreateView(\n\u001b[32m   1977\u001b[39m         child=\u001b[38;5;28mself\u001b[39m._plan, name=name, is_global=\u001b[38;5;28;01mFalse\u001b[39;00m, replace=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1978\u001b[39m     ).command(session=\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m1979\u001b[39m     _, _, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1980\u001b[39m     \u001b[38;5;28mself\u001b[39m._execution_info = ei\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1148\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations)\u001b[39m\n\u001b[32m   1146\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1147\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1152\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1560\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1557\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1537\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1811\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1809\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\plancha\\taxi-prices\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1882\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1879\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m info.metadata[\u001b[33m\"\u001b[39m\u001b[33merrorClass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1880\u001b[39m                 \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   1883\u001b[39m                 info,\n\u001b[32m   1884\u001b[39m                 status.message,\n\u001b[32m   1885\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   1886\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   1887\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1889\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/opt/spark/data/yellow_tripdata_2025-02.parquet. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:810)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... (:-1)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:1410)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:152)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:2888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2493)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)"
     ]
    }
   ],
   "source": [
    "taxi_data = spark.read.parquet(\"/opt/spark/data/yellow_tripdata_2025-02.parquet\")\n",
    "taxi_data.createOrReplaceTempView(\"taxi_data\")\n",
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b83400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>Airport_fee</th><th>cbd_congestion_fee</th></tr>\n",
       "<tr><td>2</td><td>2025-02-01 00:00:44</td><td>2025-02-01 00:05:24</td><td>1</td><td>1.04</td><td>1</td><td>N</td><td>113</td><td>107</td><td>1</td><td>6.5</td><td>1.0</td><td>0.5</td><td>9.88</td><td>0.0</td><td>1.0</td><td>22.13</td><td>2.5</td><td>0.0</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:59</td><td>2025-02-01 00:12:09</td><td>NULL</td><td>2.11</td><td>NULL</td><td>NULL</td><td>234</td><td>229</td><td>0</td><td>-4.75</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>2.54</td><td>NULL</td><td>NULL</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:59</td><td>2025-02-01 00:14:17</td><td>1</td><td>3.32</td><td>1</td><td>N</td><td>107</td><td>140</td><td>1</td><td>18.4</td><td>1.0</td><td>0.5</td><td>4.83</td><td>0.0</td><td>1.0</td><td>28.98</td><td>2.5</td><td>0.0</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:58</td><td>2025-02-01 00:25:31</td><td>1</td><td>3.4</td><td>1</td><td>N</td><td>161</td><td>148</td><td>1</td><td>23.3</td><td>1.0</td><td>0.5</td><td>2.0</td><td>0.0</td><td>1.0</td><td>31.05</td><td>2.5</td><td>0.0</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:57</td><td>2025-02-01 00:02:32</td><td>NULL</td><td>0.65</td><td>NULL</td><td>NULL</td><td>79</td><td>79</td><td>0</td><td>8.05</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>12.8</td><td>NULL</td><td>NULL</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:57</td><td>2025-02-01 00:07:11</td><td>1</td><td>1.37</td><td>1</td><td>N</td><td>163</td><td>237</td><td>1</td><td>9.3</td><td>1.0</td><td>0.5</td><td>3.01</td><td>0.0</td><td>1.0</td><td>18.06</td><td>2.5</td><td>0.0</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:57</td><td>2025-02-01 00:28:41</td><td>NULL</td><td>14.18</td><td>NULL</td><td>NULL</td><td>25</td><td>263</td><td>0</td><td>-8.44</td><td>0.0</td><td>0.5</td><td>0.0</td><td>6.94</td><td>1.0</td><td>4.49</td><td>NULL</td><td>NULL</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:56</td><td>2025-02-01 00:19:41</td><td>2</td><td>3.34</td><td>1</td><td>N</td><td>237</td><td>79</td><td>1</td><td>19.8</td><td>1.0</td><td>0.5</td><td>5.11</td><td>0.0</td><td>1.0</td><td>30.66</td><td>2.5</td><td>0.0</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:56</td><td>2025-02-01 00:06:28</td><td>NULL</td><td>1.04</td><td>NULL</td><td>NULL</td><td>164</td><td>233</td><td>0</td><td>7.25</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>12.0</td><td>NULL</td><td>NULL</td><td>0.75</td></tr>\n",
       "<tr><td>2</td><td>2025-01-31 23:59:56</td><td>2025-02-01 00:14:56</td><td>NULL</td><td>2.56</td><td>NULL</td><td>NULL</td><td>143</td><td>140</td><td>0</td><td>-4.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>4.28</td><td>NULL</td><td>NULL</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
       "|       2| 2025-02-01 00:00:44|  2025-02-01 00:05:24|              1|         1.04|         1|                 N|         113|         107|           1|        6.5|  1.0|    0.5|      9.88|         0.0|                  1.0|       22.13|                 2.5|        0.0|              0.75|\n",
       "|       2| 2025-01-31 23:59:59|  2025-02-01 00:12:09|           NULL|         2.11|      NULL|              NULL|         234|         229|           0|      -4.75|  0.0|    0.5|       0.0|         0.0|                  1.0|        2.54|                NULL|       NULL|              0.75|\n",
       "|       2| 2025-01-31 23:59:59|  2025-02-01 00:14:17|              1|         3.32|         1|                 N|         107|         140|           1|       18.4|  1.0|    0.5|      4.83|         0.0|                  1.0|       28.98|                 2.5|        0.0|              0.75|\n",
       "|       2| 2025-01-31 23:59:58|  2025-02-01 00:25:31|              1|          3.4|         1|                 N|         161|         148|           1|       23.3|  1.0|    0.5|       2.0|         0.0|                  1.0|       31.05|                 2.5|        0.0|              0.75|\n",
       "|       2| 2025-01-31 23:59:57|  2025-02-01 00:02:32|           NULL|         0.65|      NULL|              NULL|          79|          79|           0|       8.05|  0.0|    0.5|       0.0|         0.0|                  1.0|        12.8|                NULL|       NULL|              0.75|\n",
       "|       2| 2025-01-31 23:59:57|  2025-02-01 00:07:11|              1|         1.37|         1|                 N|         163|         237|           1|        9.3|  1.0|    0.5|      3.01|         0.0|                  1.0|       18.06|                 2.5|        0.0|              0.75|\n",
       "|       2| 2025-01-31 23:59:57|  2025-02-01 00:28:41|           NULL|        14.18|      NULL|              NULL|          25|         263|           0|      -8.44|  0.0|    0.5|       0.0|        6.94|                  1.0|        4.49|                NULL|       NULL|               0.0|\n",
       "|       2| 2025-01-31 23:59:56|  2025-02-01 00:19:41|              2|         3.34|         1|                 N|         237|          79|           1|       19.8|  1.0|    0.5|      5.11|         0.0|                  1.0|       30.66|                 2.5|        0.0|              0.75|\n",
       "|       2| 2025-01-31 23:59:56|  2025-02-01 00:06:28|           NULL|         1.04|      NULL|              NULL|         164|         233|           0|       7.25|  0.0|    0.5|       0.0|         0.0|                  1.0|        12.0|                NULL|       NULL|              0.75|\n",
       "|       2| 2025-01-31 23:59:56|  2025-02-01 00:14:56|           NULL|         2.56|      NULL|              NULL|         143|         140|           0|       -4.0|  0.0|    0.5|       0.0|         0.0|                  1.0|        4.28|                NULL|       NULL|               0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from taxi_data order by tpep_pickup_datetime desc limit 10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
